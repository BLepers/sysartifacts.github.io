<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<!-- Based on a template from Thomas Wenisch. Photo from https://www.flickr.com/photos/voteprime/8320401346 -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Research Artifact Evaluation @ SOSP 2019</title>
<link rel="stylesheet" type="text/css" href="./index_files/css-buttons.css">
<link rel="stylesheet" type="text/css" href="./index_files/course.css">
</head>
<body>
<div id="abstract_box">x</div>
<div id="banner" style="background-image: url(images/rube-goldberg.jpg)"></div>
<div id="header">
  <div id="details">
     <h1>Research Artifact Evaluation @ SOSP 2019</h1>
    <table class="alist">
       	<tbody>
        	<tr><th>Submission site</th><td><a href="https://sosp19ae.hotcrp.com/">
			https://sosp19ae.hotcrp.com/</a></td></tr>
        	<tr><th>Submission deadline</th><td>August 7, 2019 AoE (for Artifact Available badge)
			<br>August 14, 2019 AoE (for Artifact Functional/Results Replicated badges)</td></tr>
        	<tr><th>Technical Chairs</th><td>Baris Kasikci, University of Michigan
			<br>Vijay Chidambaram, University of Texas at Austin</td></tr>
        	<tr><th>Organizing Chair</th><td>Supreeth Shastri, University of Texas at Austin</td></tr>
        </tbody></table>
  </div>
</div>

<hr style="height:0px">
<ul id="nav">
<li><a id="home" class="good" href="index.html">Home</a></li>
<li><a id="goals" class="good" href="goals.html">Goals</a></li>
<li><a id="organizers" class="good" href="organizers.html">Organizers</a></li>
<li><a id="badging" class="good" href="badges.html">ACM Badges</a></li>
<li><a id="instructions" class="good" href="instructions.html">Instructions</a></li>
<li><a id="Results" class="good" href="results.html">Results</a></li>
<li><a id="Report" class="good" href="report.html">Final Report</a></li>
</ul>

<div id="content">

<h1>Report from the chairs</h1>

<p>We are happy to report the conclusion of the artifact evaluation (AE) process for papers accepted at SOSP 2019. Here, we share our experiences in organizing the first such effort at SOSP, and ruminate on key takeaways. Our hope is that this effort serves as a catalyst in making artifact evaluation more common at systems conferences.</p> 

<h2>Summary</h2>
<p><b>Preamble</b>: As this was the inagural year, AE was voluntary for all the accepted papers. However, we were encouraged to see that 23 of the 38 (= 61%) applied to go through the evaluation. Next, to form the artifact evaluation committee (AEC), we reached out to the broader systems community via Twitter and Slack (systems-research). This helped us bring together a <a href="organizers.html">team</a> of 43 early-career researchers and senior graduate students, who volunteered to read the papers and undertake the evaluation. Finally, in terms of artifact badges to be awarded, we gleaned over recommendations from the ACM and the PL community, and decided on <a href="badges.html">three badges</a> that made sense for systems research.</p>

<p><b>Evaluation</b>: We designed the evaluation process to be single-blinded (i.e., identity of the evaluators was not revealed to the authors). Every artifact that sought <em>Artifact Functional</em> or <em>Results Reproduced</em> badge was evaluated by two members. Unlike the paper review process, we advised the AEC members to work with the authors to help them achieve the badges they sought. This required a significant amount of communication between the evaluators and authors as well as accepting several revisions to artifacts and instructions. Due to the single-blind nature of the process, all communications were via HotCRP comments. Average length of communications including reviews and comments was XXX words per paper. The whole process starting from authors registering their artifacts, to evaluators familiarizing themselves with the underlying research papers, to verifying the artifact functionality, to reproducing the results, to writing the reviews, and to awarding the badges was completed in the span of 28 days.</p> 

<p><b>Badges</b>: Of the submitted papers, 22 (96%) earned at least one badge, and 11 (48%) earned all three badges. The authors expressed strong preferences for open-sourcing their artifacts, and to our surprise, nearly 40% of the <em>Artifact Available</em> badges were awarded to papers from industry or those that had industrial collaborators. Finally, despite having to meet rigorous standards in a short evaluation timeframe, 19 artifacts earned the <em>Artifact Functional</em> badge, and 12 earned the <em>Results Replicated</em> badge. We have tabulated detailed results along with links to artifacts <a href="results.html">here</a>.</p> 

<h2>Key Takeaways</h2>

<p>1. <em>Interest in AE is not limited to academic projects.</em> Nearly 40% of the submitted artifacts either originated from industry, or had industrial collaborators. Even when business concerns did not allow open-sourcing the artifacts, the authors were happy to let AEC members access artifacts in order to establish functionality and reproducibility.</p>

<p>2. <em>Allocate ample time and resources.</em> This SOSP AE was conceived after the paper submission deadline had passed. As a result, we had to make it voluntary, and limit its applicability to only accepted papers. We anticipate that integrating the call for artifact evaluation with the call for paper should enable much broader participation. Also, allocating dedicated hardware resources (say, as cloud credits or access to academic clouds like Chameleon) would free AEC members from the burdens of scouting for resources on their own. However, for those cases that needed specialized or expensive hardware, the authors were forthcoming to provide access to their own resources.</p>

<p>3. <em>Time is ripe for introducing AE as a regular feature in systems conferences.</em> We observed keen interest in both authors and evaluators in establishing and maintaining rigor and reproducibility in systems evaluation. This was also evident in the levels of participation, and committment to review quality all throughout the AE process. In one instance, AEC members identified a performance mismatch, which was later resolved to external dependencies. In response, the authors revised their numbers in the camera ready paper.</p>

<br><br><br>
</body></html>
